name: Airflow + Flake8 + Tests

on:
  push:
    branches:
      - main
      - 'test*'
  pull_request:
    branches:
      - main

jobs:
  lint-and-test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U airflow"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f api/requirements.txt ]; then
            pip install -r api/requirements.txt
          else
            # Fallback to basic Airflow installation if requirements file doesn't exist
            pip install apache-airflow pytest flake8
          fi

      - name: Create required directories
        run: |
          mkdir -p dags tests scripts data processed
          # Make sure scripts are executable
          if [ -d scripts ]; then
            chmod +x scripts/*.py
          fi

      - name: Run Flake8 linting
        run: flake8 dags scripts tests --max-line-length=79 --ignore=E203,W503

      # Create a docker compose configuration specifically for testing
      - name: Create Docker Compose test configuration
        run: |
          cat > docker-compose-test.yml <<EOL
          version: '3'
          services:
            postgres:
              image: postgres:13
              environment:
                POSTGRES_USER: airflow
                POSTGRES_PASSWORD: airflow
                POSTGRES_DB: airflow
              healthcheck:
                test: ["CMD", "pg_isready", "-U", "airflow"]
                interval: 5s
                retries: 5

            airflow:
              image: apache/airflow:2.7.1
              depends_on:
                - postgres
              volumes:
                - ./dags:/opt/airflow/dags
                - ./tests:/opt/airflow/tests
                - ./scripts:/opt/airflow/scripts
                - ./data:/opt/airflow/data
                - ./processed:/opt/airflow/processed
              environment:
                - AIRFLOW__CORE__EXECUTOR=LocalExecutor
                - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
                - AIRFLOW__CORE__LOAD_EXAMPLES=False
                - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
                - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
                - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
                - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30
                - AIRFLOW__CORE__LOGGING_LEVEL=INFO
          EOL

      - name: Start Docker Compose
        run: docker compose -f docker-compose-test.yml up -d

      - name: Wait for services to be ready
        run: |
          echo "Waiting for Airflow to be ready..."
          sleep 30  # Initial wait for containers to start
          max_attempts=12
          attempt=1
          while [ $attempt -le $max_attempts ]; do
            if docker compose -f docker-compose-test.yml exec -T airflow airflow db check > /dev/null 2>&1; then
              echo "Airflow is ready!"
              break
            fi
            echo "Attempt $attempt/$max_attempts: Airflow is not ready yet. Waiting..."
            sleep 10
            attempt=$((attempt+1))
          done

          if [ $attempt -gt $max_attempts ]; then
            echo "Airflow failed to become ready in time. Check logs."
            docker compose -f docker-compose-test.yml logs airflow
            exit 1
          fi

      - name: Initialize Airflow
        run: |
          docker compose -f docker-compose-test.yml exec -T airflow airflow db init
          docker compose -f docker-compose-test.yml exec -T airflow airflow users create \
            -r Admin -u admin -p admin -e admin@example.com -f admin -l admin

          # Create the pool mentioned in the DAG
          docker compose -f docker-compose-test.yml exec -T airflow airflow pools set stock_data_pool 5 "Pool for stock data tasks"

          # Make scripts executable inside the container
          docker compose -f docker-compose-test.yml exec -T airflow bash -c "chmod +x /opt/airflow/scripts/*.py"

          # List files to verify structure
          echo "Directory structure:"
          docker compose -f docker-compose-test.yml exec -T airflow bash -c "find /opt/airflow -type f | sort"

      - name: Run tests
        run: |
          docker compose -f docker-compose-test.yml exec -T airflow pytest tests/test_data_collection_dag.py -v

      - name: Show Airflow logs in case of failure
        if: failure()
        run: |
          echo "Airflow Container Logs:"
          docker compose -f docker-compose-test.yml logs airflow
          echo "DAGs Directory Contents:"
          docker compose -f docker-compose-test.yml exec -T airflow ls -la /opt/airflow/dags
          echo "Scripts Directory Contents:"
          docker compose -f docker-compose-test.yml exec -T airflow ls -la /opt/airflow/scripts

      - name: Shutdown Docker Compose
        run: docker compose -f docker-compose-test.yml down